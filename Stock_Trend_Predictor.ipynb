{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditi0255/Stock-Trend-Insight-A-News-driven-Stock-Trend-Predictor/blob/main/Stock_Trend_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GWZ1_mpnFLhm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ssz3LNp5FLho",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "5fba3c9d-c2f9-48d2-caf2-b75db0b76403"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-40b12d35cf76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Combined_News_DJIA.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Combined_News_DJIA.csv'"
          ]
        }
      ],
      "source": [
        "data=pd.read_csv('/content/Combined_News_DJIA.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZJFbwPrFLho"
      },
      "outputs": [],
      "source": [
        "#Dataset contaning the Top 25 news headlines on the day\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywF8T8cI7tB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2ormkUWI9LM"
      },
      "outputs": [],
      "source": [
        "data = data.replace(np.nan, ' ', regex=True)\n",
        "data.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZr37n4vJB1H"
      },
      "outputs": [],
      "source": [
        "data = data.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObP5loysFLhp"
      },
      "outputs": [],
      "source": [
        "data2=pd.read_csv(\"/content/upload_DJIA_table.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO7sX4aEFLhp"
      },
      "outputs": [],
      "source": [
        "#This contains the market Open,High,Low,Close ,Volume and Adj Close\n",
        "data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IZEfpAzFLhp"
      },
      "outputs": [],
      "source": [
        "#Combining the data set with the Market price movement\n",
        "merge=data.merge(data2,how=\"inner\",on=\"Date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a8lc9uMFLhq"
      },
      "outputs": [],
      "source": [
        "merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dczDXfhtFLhq"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgFgx7QbFLhq"
      },
      "outputs": [],
      "source": [
        "# merge.profile_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ZQRmjMFLhq"
      },
      "outputs": [],
      "source": [
        "headline=[]\n",
        "for row in range(0,len(merge.index)):\n",
        "    headline.append(\" \".join(str(x) for x in merge.iloc[row,2:27]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_q6ekCEFLhq"
      },
      "outputs": [],
      "source": [
        "headline[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ljUCrbVFLhr"
      },
      "source": [
        "re.sub(pattern, repl, string, count=0, flags=0)\n",
        "\n",
        "The re.sub() function stands for a substring and returns a string with replaced values. Multiple elements can be replaced using a list when we use this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHE0yUwUFLhs"
      },
      "outputs": [],
      "source": [
        "clean_headline=[]\n",
        "for i in range(0,len(headline)):\n",
        "    clean_headline.append(re.sub(\"b[(')]\",'',headline[i])) #remove b'\n",
        "    clean_headline[i]=re.sub('b[(\")]','',clean_headline[i]) #remove b\"\n",
        "    clean_headline[i]=re.sub(\"\\'\",'',clean_headline[i]) #remove \\'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEparzJWFLhs"
      },
      "source": [
        "Adding the combined and cleaned news back to the merge table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3ExnJ2RFLhs"
      },
      "outputs": [],
      "source": [
        "merge['Combined_News'] = clean_headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHKlR19ZFLhs"
      },
      "outputs": [],
      "source": [
        "merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNnRlhovFLht"
      },
      "outputs": [],
      "source": [
        "#Alternative approach more traditional NLP approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6qSE2FAFLht"
      },
      "outputs": [],
      "source": [
        "samples = list(merge['Combined_News'][:5].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jro_zbzBFLht"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEWJ5Be8FLht"
      },
      "outputs": [],
      "source": [
        "token_index = {}  # builds an index of tokens in the data\n",
        "for sample in samples:\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtHlwcZhFLht"
      },
      "outputs": [],
      "source": [
        "max_length = 15\n",
        "\n",
        "results = np.zeros(shape=(len(samples),   # results will be stored in this array\n",
        "                          max_length,\n",
        "                          max(token_index.values()) +1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZJDKxFWFLhu"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of stored results array:\", results.shape)\n",
        "print(\"Token index of unique words: \\n\", token_index)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i,j,index] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPPpu3DFFLhu"
      },
      "outputs": [],
      "source": [
        "# one hot encoding using keras tokenizer and pad sequencing\n",
        "X = merge['Combined_News']\n",
        "encoder = LabelEncoder()\n",
        "# y = encoder.fit_transform(merge['Combined_News'])\n",
        "y=merge['Label']\n",
        "print(\"shape of input data: \", X.shape)\n",
        "print(\"shape of target variable: \", y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT8LQUeuFLhu"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import nltk\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66jKxsRxFLhu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def remove_accented_characters(text):\n",
        "    text =  unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def contractions_text(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    filtered_sentence = []\n",
        "\n",
        "    for w in word_tokens:\n",
        "\n",
        "        if w not in stop_words:\n",
        "\n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2eO9-YWFLhu"
      },
      "outputs": [],
      "source": [
        "def preprocessor_engine(text):\n",
        "    corpus =[]\n",
        "    for sent in tqdm.tqdm(text):\n",
        "        sent = remove_accented_characters(sent)\n",
        "        sent = stop_words(sent)\n",
        "        corpus.append(sent)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc17zqDAKB65"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXCb-3xKFLhu"
      },
      "outputs": [],
      "source": [
        "train_data_pro = preprocessor_engine(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNFG9P_2FLhv"
      },
      "outputs": [],
      "source": [
        "test_data_pro = preprocessor_engine(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9n6THJoFLhv"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bdzYTpTFLhv"
      },
      "outputs": [],
      "source": [
        "articles_list=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9KsEzP9FLhv"
      },
      "outputs": [],
      "source": [
        "cat_array = train_data_pro # array of news articles text in each category\n",
        "for i in range(len(cat_array)):\n",
        "    articles_list.append(cat_array[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-t_ZEQFLhv"
      },
      "outputs": [],
      "source": [
        "wc1 = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='black',\n",
        "               contour_color='black',\n",
        "               colormap='plasma',\n",
        "               repeat=True,\n",
        "               stopwords=STOPWORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uy3hIaYFLhv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for id in range(len(articles_list)-1):\n",
        "    plt.subplot(5, 2, id % 10 + 1)\n",
        "    cloud = wc1.generate(' '.join(articles_list[id]))\n",
        "    plt.imshow(cloud, interpolation= \"bilinear\")\n",
        "    plt.title(f\"Wordcloud for news topic {id}\")\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiP7HeUoFLhw"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer( oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(train_data_pro) # build the word index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NyBMeZMFLhw"
      },
      "outputs": [],
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_data_pro)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data_pro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY3XenG7FLhw"
      },
      "outputs": [],
      "source": [
        "# pd.Series(train_data_pro).apply(lambda x : len(x.split())).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGnRKpRxFLhx"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 424\n",
        "\n",
        "train_pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')\n",
        "test_pad_sequneces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xkL4-zBFLhx"
      },
      "outputs": [],
      "source": [
        "total_words=len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW2Qkjz1FLhx"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
        "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import L1L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxoUpETOFLh6"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 300, input_length=MAX_SEQUENCE_LENGTH ))\n",
        "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
        "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
        "model.add(SimpleRNN(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz8s9ecfFLh6"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, SimpleRNN, Dropout, Dense\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(train_data_pro)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data_pro)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data_pro)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 424\n",
        "\n",
        "train_pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "test_pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1  # Add 1 for the padding index\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 300, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
        "model.add(Bidirectional(SimpleRNN(64, dropout=0.1, recurrent_dropout=0.30, activation='tanh', return_sequences=True)))\n",
        "model.add(SimpleRNN(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5zMPykVAfPkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETC2EqCeFLh6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "history = model.fit(train_pad_sequences , y_train,\n",
        "                   batch_size=128,\n",
        "                    epochs=2,\n",
        "                    validation_split=0.1\n",
        "                   )\n",
        "\n",
        "#evalute the model\n",
        "test_loss, test_acc = model.evaluate(test_pad_sequneces, y_test, verbose=0)\n",
        "print(\"test loss and accuracy:\", test_loss, test_acc)\n",
        "\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "# train_data = tf.data.Dataset.from_tensor_slices((train_padseq, y_train))\n",
        "# valid_data = tf.data.Dataset.from_tensor_slices((test_padseq, y_test))\n",
        "\n",
        "# model.fit(train_data, epochs=10, validation_data=valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P6hfonSFLh6"
      },
      "outputs": [],
      "source": [
        "def get_subjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRpFrFm2FLh6"
      },
      "source": [
        "TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8T-B6ORFLh6"
      },
      "outputs": [],
      "source": [
        "#Finding the Subjitivity and polarity score from the data set\n",
        "merge['Subjectivity'] = merge['Combined_News'].apply(get_subjectivity)\n",
        "merge['Polarity'] = merge['Combined_News'].apply(get_polarity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "metadata": {
        "id": "-BDqp0r9jJEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-KghOdLFLh7"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJsZITxzFLh7"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk5XSRiOFLh7"
      },
      "source": [
        "Vader Sentiment Analysis helps is putting out the score as overall sentiment score as positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E63wJ3kbFLh7"
      },
      "outputs": [],
      "source": [
        "def getSIA(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment= sia.polarity_scores(text)\n",
        "    return sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD1CWsdUFLh7"
      },
      "outputs": [],
      "source": [
        "compound=[]\n",
        "neg=[]\n",
        "pos=[]\n",
        "neu=[]\n",
        "SIA=0\n",
        "\n",
        "for i in range (0, len(merge['Combined_News'])):\n",
        "    SIA= getSIA(merge['Combined_News'][i])\n",
        "    compound.append(SIA['compound'])\n",
        "    neg.append(SIA['neg'])\n",
        "    pos.append(SIA['pos'])\n",
        "    neu.append(SIA['neu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smJCsXCkFLh7"
      },
      "outputs": [],
      "source": [
        "merge['compound']= compound\n",
        "merge['neg']= neg\n",
        "merge['pos']=pos\n",
        "merge['neu']=neu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzSnk9bmFLh7"
      },
      "outputs": [],
      "source": [
        "df= merge[['Label','Open', 'High', 'Low','Volume','Subjectivity','Polarity','compound','neg','pos','neu']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wLPBY7MFLh8"
      },
      "outputs": [],
      "source": [
        "X=df\n",
        "X=np.array(X.drop(['Label'],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv-1-iNMFLh8"
      },
      "outputs": [],
      "source": [
        "y= np.array(df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kBqp6CtFLh8"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state= 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ghGdME1FLh8"
      },
      "outputs": [],
      "source": [
        "model= LinearDiscriminantAnalysis().fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drU6ALWrFLh8"
      },
      "outputs": [],
      "source": [
        "predict= model.predict(x_test)\n",
        "\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-plot\n",
        "\n"
      ],
      "metadata": {
        "id": "sMgBNwvyjx1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhOB29mVFLh8"
      },
      "outputs": [],
      "source": [
        "from scikitplot.metrics import plot_confusion_matrix as plt\n",
        "plt(y_test,predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Ci1QKsFLh9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY9UWglDFLh9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 129,
          "sourceId": 792900,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30407,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}